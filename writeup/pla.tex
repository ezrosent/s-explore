\documentclass[12pt]{article}
\usepackage{graphicx,datetime,fancyhdr,amsmath,amssymb,amsthm,subfig,url,hyperref}
\usepackage{paralist,semantic,minted,latexsym}
\usepackage{titling,float,multicol,setspace,titlesec,framed}
\usepackage[margin=1in]{geometry}
%----------------------- Macros and Definitions --------------------------

\renewcommand{\theenumi}{\bf \Alph{enumi}}
\setlength{\parindent}{0.3cm}
\setlength{\parskip}{0.0cm}
\fancypagestyle{plain}{}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO,RE]{\sffamily\bfseries\large Semantics Exploration}
\fancyhead[RO,LE]{\sffamily\bfseries\large Eli Rosenthal}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
\graphicspath{{figures/}}
\setlength{\droptitle}{-1cm}
\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}
\newcommand{\com}[2]{\lbrack#1,#2\rbrack}
\newcommand{\ns}{\triangleleft}
\newcommand{\co}[2]{#1 #2 #1^{-1}}
\newcommand{\aut}{\text{Aut}}
\newcommand{\GL}{\text{GL}}
\newcommand{\SL}{\text{SL}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ob}{\textit{Ob}}
\newcommand{\csa}{\textsc{CS-A}}
\newcommand{\csb}{\textsc{CS-B}}
\newcommand{\uf}[1]{\textsf{unfold}~[#1]}
\newcommand{\fl}[1]{\textsf{fold}~[#1]}
\newcommand{\R}{\mathcal{R}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\L}{\mathcal{L}}

\renewcommand{\S}[1]{\textsc{#1}}
\renewcommand{\hom}{\textit{Hom}}
\mathlig{|->}{\mapsto}
\mathlig{~>}{\leadsto}
\mathlig{<<<}{\sqsubseteq}
%-------------------------------- Title ----------------------------------
\begin{document}

\begin{abstract}

We define a notion of \emph{surprise} between two programming languages which
provides a formal analog to the notion that ``A user experienced with Scheme is
surprised by feature $x$ of Javascript''. Along the way we give clear
definitions of what it means for a language feature to be \emph{orthogonal}, as
well as provide an automated tool for exploring (in a purely syntactic manner)
the extent to which a language implements a feature in an unusual way.

\end{abstract}

\section{Introduction: Two Parables}

We begin considering an idealized version of two common cases of ``programming
language aquisition.''


\subsection{The Intro CS Student}

Consider a student X taking an introductory programming course (\csa). X has no prior
programming experience, and their first assignment is to use their language
$L_0$ as a calculator. $L_0$ is a small ``expression language'' with numbers and
basic arithmetic operations. 

\begin{minted}[mathescape=true]{scheme}
;; Assignment 1
; Compute $\sqrt{157482506564341206945618310642111308294618078190384348521}$
(sqrt 157482506564341206945618310642111308294618078190384348521)
; => 12549203423498290193999432211
\end{minted}

In their next assignment, X now has access to conditional branching and
functions. To use these features, X writes their assignment in $L_1$, which also
has all of the features of $L_0$.

\begin{minted}[mathescape=true]{scheme}
;; Assignment 2
; Write a function which given two numbers $x,y$ computes $\sqrt{xy}$
(define (f x y) (sqrt (* x y)))
\end{minted}

While X views this assignment as building on previous techniques, they don't
realize that the semantics of \verb|sqrt| and \verb|*| have changed!  In $L_0$,
they were special forms built into the language, and in $L_1$ they are
implemented as functions in the standard library. X may not have noticed this
because there are few \emph{observations} that they could make on language output
that could distinguish these two scenarios. 

The following semester, X's instructors have decided they have to learn a
``practical language'' because X and their classmates have to be more attractive
to employers, or some other bullshit reason; X's class (\csb) is hence taught in
python! Python has different syntax than the $L_i$ that X learned in \csa, so X
does some of the \csa~assignments and tries to translate them to python.

\begin{minted}[mathescape=true]{python}
# Assignment 1
from math import sqrt
sqrt(157482506564341206945618310642111308294618078190384348521)
# => 1.254920342349829e+28
# Assignment 2
def f(x):
    return sqrt(x * y)
\end{minted}

Both of these examples surprised X! In assignment 1, the first answer is wrong
(python's \verb|math.sqrt| does not provide an exact answer). In assignment 2, X
made a mistake. However, X only realized their mistake after running a test
case, while $L_{i \geq 1}$ produced a static (``compile-time'') error if there
are free variables in a function body\footnote{We note that the Full Monty paper
exhibits many more python features that are surprising in this sense. A
seasoned schemer may be surprised by some edge-cases in python generators that
make them hard to interpret through a straight-forward CPS transform (?).}.


\subsection{The Programming Languages Expert}

Whenever a new language begins to gain traction, it is common for a PL theorist
to play around with pathological examples of programs that behave
``unexpectedly.''\footnote{There is a variant of this parable in which someone
  who is experienced using a language is surprised by some edge-case in the
  language: demonstrating that their mental model of the language was not fully
  in line with the implementation. We can call these variants ``Wat talk''
  variants} 
This is often a rather useful exercise: it can help programmers avoid
hard-to-diagnose bugs\footnote{Shriram Krishnamurthy's Google+ posts
with \#swiftlang are probably in this category.}, and it is often a first step to
developing a practical or \emph{tested semantics} for a language without a
formal specification. Such specifications, in turn, can lead to type systems and
static analyses for the language (hence catching more bugs)\footnote{Joe's
Javascript types paper was used in Facebook's Flow, right?}.

\subsection{When are Languages Surprising?}

We observe that the two instances cases considered above are both instances of
the same notion of surprise: a programmer has some underlying model of how evaluation
in the language ought to proceed, but the language does something else. In the
case of the PL theorist, the language isn't $L_1$ or $L_0$\footnote{Mainly
  because these languages don't exist, though they bare some resemblance with the HtDP
  teaching languages.}; it may be some
variant of the $\lambda$-calculus. Moreover, as the first parable alludes to, it
is possible to have languages with respect to which some other languages are
\emph{not} surprising. $L_1$, while it had different semantics for $L_0$'s
features, was a more comfortable next step than python. Another way of stating
this is that the features that $L_1$ added to $L_0$ are \emph{orthogonal} to the
preexisting features.

\section{A Semantic Definition of Surprise}
% evaluation rules, semantics etc.

In this section we identify a language $\L$ with its \emph{reduction semantics}:
i.e.~the tuple $(\R_\L, \E_\L, ~>_\L, =>_\L)$ denoting the set of a set of
redexes, evaluation contexts, reduction relation, and transition relation,
respectively.\footnote{Using the terminology of \textit{Design Concepts in
Programming Languages}}

We say $\L'$ is a \emph{sublanguage} of $\L$ (denoted $\L' \subseteq \L$) if
each component of $\L'$ is a subset of its corresponding component in $\L$.
$\L_1$ \emph{partially models} $\L_2$ if there are functions $m_1 : \R_{\L_1} ->
\R_{\L_2}$ and $m_2 : \E_{\L_1} -> \E_{\L_2}$.

Note that the existence of a partial model induces a sublanguage in the language
being modeled, given a model $m=(m_1,m_2)$ we can denote such a sublanguage $m(\L_1)$.
Given a partial model $m : \L_1 -> \L_2$  we therefore have another language
$\L_{1 -> 2}$ given by

\textbf{Note: this is actually an easier definition when we just think of the
single reduction relation in a structural operational semantics}

\begin{align*}
\intertext{(This is the general idea, but something may be messed up here)}
  \R_{\L_{1 -> 2}} &= \R_{L_2} \\
  \E_{\L_{1 -> 2}} &= \E_{\L_2}  \\
  ~>_{\L_{1 -> 2}} &= \{(r, r')~:~r ~>_{\L_2}r' \text{ and } r \not\in m(\L_1)\} \cup 
  \{(m_1(r), m_1(r'))~:~r~>_{\L_1}r'\}\\
  =>_{\L_{1 -> 2}} &= \{(e\{r\}, e\{r'\})~:~r,r' \in \R_{\L_2}.~e\{r\} =>_{\L_2}e\{r'\} \text{ and } e \not\in m(\L_1)\}\\ 
  & \cup 
  \{(m_2(e)\{m_1(r)\}, m_2(e)\{m_1(r')\})~:~r,r' \in m_1(\R_{\L_1})~e\{r\}=>_{\L_1}e\{r'\}\}
\end{align*}

We say that $\L_2$ is \emph{surprising with respect to $\L_1$} if $=>_{\L_{1 ->
    2}} \cup =>_{\L_2}$ yields a non-deterministic\footnote{We assume that the
  semantics were deterministic beforehand; accounting for non-determinism is
  doable, it's just harder to write down.} relation. $\L_{1->2}$ is an alternative
semantics for the syntax of $\L_2$; it is the ``mental model'' that a programmer
might have of $\L_2$. This creates surprises when letting $\L_{1->2}$ reduce an
expression for any number of steps before handing the expression back to the
$\L_1$ semantics yields a different result than just evaluating all reduction
steps in $\L_1$.\footnote{TODO: add a helpful diagram} If $\L_2$ is not
surprising with respect to $\L_1$, then we say language constructs present in
$\L_2$ but not in $m(\L_1)$ are \textit{orthogonal} to those in $m(\L_1)$.

\section{How this might look}

There are two options for how this could look in practice. Say we want to assess
the implementation of scoping for a particular language with respect to that of
the untyped $\lambda$-calculus. There are two ways that I see we could do this.
Jack mentioned something about the expressive power of languages as stated in
terms of \emph{what you can resugar}. That notion sounds directly applicable to
this problem.

\subsection{The Easy Way}

Take terms that only include UTLC-like features. This would mean very basic
functions (depending on the mapping, probably just function application, and
maybe assignment), and either evaluating them to completion (in the big-step
setting) or replacing them with an equivalent term (in the small-step setting).
This is less likely to have strange edge-cases than the option below, but
things like free variables are hard in this case, so we still can't throw errors
related to unbound identifiers. The bigger issue is that we narrow the scope of
our search, potentially losing out on useful examples of how certain features
interact with one another.

Open questions here include whether we can get away with doing this for
sub-terms (the answer seems yes for pure languages, and no for anything with
state), or if we have to do this for full programs only (in which case it
becomes less interesting). We don't have this issue as much in the following
approach.

\subsection{The Cooler-but-might-not-work way}

Allow for the inclusion of terms that we do not understand (arrays, indexing,
objects) but map them to \emph{uninterpreted values}. We can then treat these
uninterpreted values like free identifiers (not having an LHS for an evaluation
relation) and map them back as before. For example, given the following:

\begin{minted}{python}
class Foo(object): ...

def foo(bar):
  x = Foo(bar)
  return x.baz(foobar)
foobar = 3
foo(4)
\end{minted}

We could map this program to a variant of the $\lambda$-calculus with
sequencing (useless here, as there is no mutation) and \texttt{let} (using scheme
syntax here):

\begin{minted}{scheme}
(do
  (uninterp (class Foo(object): ... ))
  (let ((foo (lambda bar
              (let ((x (Foo bar)))
                ((. x baz) foobar))))
        (foobar 3))
    (foo 4)))
\end{minted}

In such a way that we have an inverse that gives us back the same program. I
think it's pretty clear how that would work in this case. Evaluating this
code to completion should return the same context, but with the \texttt{(foo
4)} statement replaced with.

\begin{minted}{scheme}
(let ((x (Foo bar)))
  ((. x baz) foobar1))
\end{minted}

Where \texttt{foobar1} is a fresh identifier. This code no longer runs to
completion in python, so we have discovered something interesting! This also
gives us a much simpler algorithm for searching for a surprise, we just
evaluate as much of the expression as we can and map it back in.

\textbf{But there is a problem!} What if we had named the class
\texttt{foobar1}\footnote{While it is a common convention in python to start
  function names with a lower-case letter and class names with upper-case,
  this isn't true for, say, C-like languages where it depends on the style
guide}? Then our renaming would cause a previously free identifier to be
bound. There doesn't appear to be an easy way to avoid this for languages
where different features share a namespace. We could attach asterisks to
cases such as these, or we could try to grep the source of the original
program for the identifiers we generate. Many possible work-arounds, but it
doesn't sound clean. One way to phrase this in terms of the language above
is that we are not modelling all of a specific feature (like scope) by
mapping this to the $\lambda$-calculus without classes. On the other hand
maybe this is a good thing?


% \section{Practical Implementation}
% % describe the implementation effort with respect to the lambda calculus

% To make this definition practical, we can restrict ourselves to operating on the
% syntax of the language being modeled, provide syntactic mappings to a language
% \emph{with} a semantics (e.g.~the $\lambda$-calculus), and non-deterministically
% substitute equivalent terms in the second language into their counterparts in
% the first. Given an interpreter for the modeled language, we can then search for
% surprises in an automated fashion.

\section{Current Implementation}

Here we detail the current Haskell implementation of the sketches above. We
assume that a language is given as an Algebraic Data-type (ADT) with the
recursive knot untied, namely, given an expression language like:

\begin{minted}{haskell}
data Exp = Number Integer | Add Exp Exp
\end{minted}

We expect it to be given as
\begin{minted}{haskell}
data Exp a = Number Integer | Add a a
\end{minted}

Which is a straightforward transformation. Given this unrolled representation,
it is possible to ``tie the knot'' manually with a \texttt{newtype}-ed
Y-combinator, like so:
\begin{minted}{haskell}
newtype Mu f = Mu (f (Mu f))

type Exp' = Exp (Mu Exp)
\end{minted}

We use a simple extension to this, which to our knowledge is new, to allow for
two ADTs to be interleaved:

\begin{minted}{haskell}
newtype CR a b = CR (Either (a (CR a b)) (b (CR a b)))
type a :+: b = CR a b
\end{minted}

We then use the popular \texttt{lens} library to execute the following workflow,
given a language to model $\L_1$ and a language for which we have rules
regarding referential transparency ($\L_0$).

\begin{enumerate}[(1)]
\item Write rules translating $\L_1$ variants into $\L_0$ ones. This need only
  have the type \texttt{a :+: b -> a :+: b}, so they should be shallow, and then
  \texttt{lens} can apply the rules until it reaches a fixed point. Inverse
  rules should also be written
  
\item ``Fuzz'' using a QuickCheck Arbitrary instances, the subterms in $\L_0$
  for referentially transparent ones (e.g.~$\beta$-reduction/expansion)
  
\item Apply the inverse rules, compare the output(s) to evaluating the
  unadulterated expression.
\end{enumerate}

Note that the inverse rules and mapping rules are very similar to the platonic
ideal of just explaining what AST nodes are functions. This framework also
allows for easily testing (using Quickcheck properties) whether or not the
mappings are true inverses invariant under the substitutions made by step 2.

\section{Orthogonality}

A special case of the notion of sublanguage described above is the notion of
feature interaction. It is considered desirable for a language feature to be
implemented in an \emph{orthogonal} manner, a term which is informally taken to
mean ``doesn't cause other features in the language to behave
unexpectedly''\footnote{Another use-case is that of redundancy. Feature X is not
orthogonal to feature Y if we can accomplish some class of programs with either
X or Y. I don't think we cover that here}. We can model this has partitioning a
given language into the language with and without the feature, and consider
surprising cases in that context. While this may appear to be a mere exercise,
it is very similar to the process of testing a new language implementation
against preexisting programs.

The purpose of this section, however, is to show how it is possible to provide a
language implementation that is orthogonal by construction; in fact,
orthogonality will be a free theorem, assuming a program is written with the
proper type signature. Consider an ADT for an untyped $\lambda$-calculus with
simple types:

\begin{minted}{haskell}
data Exp 
  = Id String 
  | Lam String Exp 
  | App Exp Exp 
  | ArithOp (Integer -> Integer -> Integer) Exp Exp 
  | N Integer
  | BoolOp (Bool -> Bool -> Bool) Exp Exp
  | B Bool
  | IF Exp Exp Exp
\end{minted}

% We can now annotate this ADT with feature-specific information, where
% \texttt{:++} is append for type-level lis% t

We can instead break this out into separate datatypes:

\begin{minted}{haskell}

-- Empty phantom types denoting present features
% data Lang = UTLC | Booleans | Arithmetic
data UTExp a  = Id String | Lam String a | App a
data ArithExp a  = N Integer | ArithOp (Integer -> Integer -> Integer) a
data BoolExp a  = B Boolean | BoolOp (Boolean -> Boolean -> Boolean) a
data Exp  where
   AE :: ArithExp Exp -> Exp 
   BE :: BoolExp  Exp -> Exp 
   UE :: UTExp    Exp -> Exp 
  
-- Existentially quantified expression type
data SomeExp = forall ls. SE (Exp ls)
data Val = NV Integer | BV Boolean | Clos SomeExp 
\end{minted}

We can then implement an interpreter feature-by-feature using a variant of the ``extensible
\verb|letrec|'' trick: Here is how we would implement the \texttt{Arithmetic} interpreter.

\begin{minted}{haskell}
arithEval :: (a -> Maybe Val) -> ArithExp a -> Maybe Val
\end{minted}
\begin{minted}{haskell}
arithEval _    (N n)           = Just (NV n)
arithEval eval (ArithOp f a b) = do 
  a' <- eval a
  b' <- eval b
  case (a', b') of (Just (NV x), Just (NV y)) -> return $ NV $ f x y
                   _ -> Nothing
\end{minted}
\begin{minted}{haskell}
boolEval :: (a -> Maybe Val) -> BoolExp a -> Maybe Val
-- ...
\end{minted}
\begin{minted}{haskell}
utlcEval :: (a -> Maybe Val) -> UTExp a -> Maybe Val
-- ...
\end{minted}
\begin{minted}{haskell}
eval :: Exp -> Maybe val
eval (AE a) = arithEval eval a
eval (BE a) = boolEVal eval a
eval (UE a) = utlcEval eval a
\end{minted}

Note now we can state a clean notion of sublanguage. A sublanguage is just an
ADT with some subset of the possible features (i.e. some of the variants of
\texttt{Exp}). To prove orthogonality, we have to say that the evaluation of any
sublanguage is compositional: it is identical regardless of what additional
features are in the language. But this is just a free theorem given the type
signature of the \texttt{*Eval} functions; they cannot possibly have an impact
on additional features because they cannot inspect the abstract type parameter
\texttt{a}! The remainder of the proof is to verify that \texttt{eval} does the
obvious thing with each of these helper functions: a process that could easily
be automated.

Hence, we have provided a scheme for building interpreters for languages that
are orthogonal \emph{by construction} and implement this without the need for
full dependent types.

\paragraph{Formalizing Sublanguage in the type system} We could decorate
expression variants with type-level lists, we may maintain parametricity but may
require fancy rank-n types. Worth considering soon.


% \paragraph{Doubts/Concerns} There is something fishy about using parametricity when
% considering GADTs. Each of these helper functions can just pattern-match on
% all of the values of subterms. However, we could still get parametricity if we
% insist that they live in a separate module, and not require to be recompiled if
% \texttt{Exp} is augmented, but then we would require a stronger module system
% than what Haskell currently has. I think we are close to something useful
% though. Unrolling the data-type definition could help.


\section{Denotational Setting}
We can give the same definition in the context of denotational semantics. This
gives a more concise characterization of surprise in terms of commutative
diagrams.

% describe the "Kan extension" at work here. This relates to what Shriram
% mentioned with big-step vs small-step.


\end{document}
